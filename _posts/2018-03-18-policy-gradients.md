---
layout:     post
title:      Introduction to Policy Gradients
date:       2018-03-18 11:21:29
summary:    Visual introduction to the concepts underlying modern Deep-RL methods
categories: RL, Deep RL, Policy Gradients
---

<h3 id="concept">Concept</h3>
<p>Policy gradients are a class of reinforcement learning methods where we attempt to optimize a policy directly, as opposed to extracting it from value functions. For example, in Q-Learning we iteratively fit (using SGD or similar method) a parameterized function <span class="math inline">\(Q_\theta(s,a)\)</span> to a target which we are also iteratively updating using some variant of the bellman equation: <span class="math inline">\(Q_\theta(s_t,a_t) = r_t + \gamma \max_a Q_\theta(s_{t+1},a)\)</span>. The policy, <span class="math inline">\(\pi(s,a) = P(a|s)\)</span>, which actually decides what we are going to do at any time-step, is then constructed implicitly from these learned Q-values, usually by <span class="math inline">\( \operatorname*{arg\,max}_{a} Q(s_t, a)\)</span>. Clearly there are a lot of problems with this; how do you sample from the environment, how many parameter updates do you perform between bellman updates, how do you keep the Q-estimates stable etc. These are all problems that the Deep Q-Network algorithm addresses, using things like experience replay and target networks.</p>
<p>Policy Gradients sidestep these problems entirely by parameterizing the policy itself, and updating the parameterization over successive rollouts in order to converge on an optimal policy. Specifically, we try to learn a parameterized mapping <span class="math inline">\(\pi_\theta: \mathcal{S}\to \mathcal{A}\)</span> where exploration is implicit and the argmax step is not required.</p>

_![desk]({{ "/images/PG.png" | absolute_url }})_
*Vanilla Policy Gradients.* Here a simplified case is shown where all policy parameters, except two, are fixed - allowing for the visualization of the expected total reward of the parameterization space as a 3D (policy) surface. In reality this surface has dimensionality equal to the number of parameters and is unlikely to be smooth. Any point on this surface corresponds to specific weights, $\theta_0$ and $\theta_1$ in the policy network. Once the policy network is fixed, rollouts can be generated by moving through the environment following the policy. One rollout is shown here, with the bars to the right of each state indicating the probability of taking each action as dictated by the policy. Grey bars indicate actions that are selected for that specific rollout. Note that in VPG this policy is stochastic, and so exploration is implicit. During interaction, the rewards ($r_ht$) returned by the environment are recorded and used, along with the action probabilities, $\pi(s_t|a_t)$, to calculate the policy gradient. The parameters of the policy are then updated by a certain amount (determined by the decent algorithm e.g. SGD) and the process is repeated.

<h3 id="background-1">Background</h3>
<p>Vanilla Policy Gradients (VPG) refer to the REINFORCE class of algorithms first proposed by Sutton et. al. in 1999 <span class="citation">\cite{Sutton1999a}</span>. In these algorithms we assume a stochastic parameterized policy <span class="math inline">\(\pi_\theta(s,a)\)</span> , and a trace (rollout) from this policy <span class="math inline">\(\tau = \{ s_0, a_0 ,..., s_k, a_k \}\)</span>. The total reward of this trace is then; <span class="math inline">\(R(\tau) = \sum^k_{t=0} r(s_t, a_t)\)</span> where <span class="math inline">\(r(s_t, a_t)\)</span> is the reward received by being in state <span class="math inline">\(s_t\)</span> and taking action <span class="math inline">\(a_t\)</span><a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. In order to do policy optimization, we need a metric over which to optimize, which in the RL setting is the total expected reward of our policy. <span class="math display">\[U({\pi_\theta}) = \mathbb{E}_{\tau \sim \pi_\theta} {R(\tau}) = \sum_\tau P(\tau|\pi_\theta) R(\tau)\]</span></p>
<p>We then want to find <span class="math inline">\(\operatorname*{arg\,max}_\theta U(\pi_\theta)\)</span>, and we would like to do this with a gradient-informed search. In other words, we want to find the vector <span class="math inline">\(\nabla_\theta U(\pi_\theta) = \left( \frac{\delta U(\pi_\theta)}{\delta \theta_0}, \frac{\delta U(\pi_\theta)}{\delta \theta_1}, , ..., \frac{\delta U(\pi_\theta)}{\delta \theta_n}   \right)^\top \)</span>. Where <span class="math inline">\(n\)</span> is the number of parameters. This is the policy gradient. There are numerous ways to compute this, the simplest being to perturb one parameter slightly, and observe the difference <span class="math inline">\( U(\pi_\theta) - U(\pi_{\theta_\text{perturbed}})\)</span> as the gradient, repeating for each parameter. This is the method of finite differences<span class="citation">\cite{Glynn1987}</span>, which while simple conceptually is hugely inefficient. It does have the advantage, however, of not requiring <span class="math inline">\(\pi_\theta\)</span> to be differentiable, meaning it can work with any parametrization method, not just neural networks.</p>
<p>If we constrain ourselves to the case where <span class="math inline">\(\pi_\theta\)</span> is differentiable, however, we can compute the gradient analytically as;</p>
<p><span class="math display">\[\begin{aligned}
\nabla_\theta U(\pi_\theta) &amp;= \nabla_\theta \sum_\tau P(\tau|\pi_\theta) R(\tau) \\
&amp; =  \sum_\tau \nabla_\theta P(\tau|\pi_\theta) R(\tau) &amp;&amp; \text{gradient of sums = sum of gradients}\\
&amp; =  \sum_\tau P(\tau|\pi_\theta) \frac{\nabla\theta P(\tau|\pi_\theta)}{P(\tau|\pi_\theta)} R(\tau) &amp;&amp; \text{multiply by 1} \\
&amp; =  \sum_\tau P(\tau|\pi_\theta) \nabla_\theta \log P(\tau|\pi_\theta) R(\tau) &amp;&amp; \text{identity:}  \frac{\nabla a}{a} = \nabla \log a \\
&amp; = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \nabla_\theta \log P(\tau|\pi_\theta) R(\tau) \right] &amp;&amp;\text{def. of expectation}
\end{aligned}\]</span></p>
<p>This derivation makes use of the log-derivative trick, which also shows up in many other areas of ML. If we have <span class="math inline">\(k\)</span> traces from <span class="math inline">\(\pi_\theta\)</span>, then we can reframe our gradient as a Monte Carlo estimate in the form; <span class="math display">\[\nabla_\theta  U(\pi_\theta) \approx \hat{g} = \frac{1}{k} \sum_{i=1}^k \nabla_\theta \log P(\tau^{(i)}|\pi_\theta) R(\tau^{(i)})\]</span></p>
<p>In simple terms; take a bunch of traces, get their cumulative reward, then multiply this reward by the gradient of the log probability of that trace occurring under our current policy, then average this across all traces. We weight the gradient of the probability of a trace occurring by it’s cumulative reward. Our parameterization then makes traces with high rewards more likely if it moves along this gradient, which is exactly what we want.</p>
<p>This approach is not without problems however. While we can get cumulative rewards directly from the environment, how can we calculate how likely a trace is to occur (the <span class="math inline">\(P(\tau^{(i)}|\pi_\theta)\)</span> term) under <span class="math inline">\(\pi_\theta\)</span>? Additionally, this formulation will cause all traces with positive rewards to increase in probability, when really we want the worse traces to become more unlikely.</p>
<p>Calculating the probability of a trace has an easy solution because we don’t actually care about this probability, we only care about it’s log gradient with respect to our parameters. <span class="math inline">\(P(\tau^{(i)}|\pi_\theta)\)</span> is equal to the probability of the trace being generated by the policy, times the internal system dynamics (note: to show this rigorously, we have to expand everything out to state-action pairs). However, internal system dynamics are not effected by our policy parameterization, so we can ignore them. Consequently, <span class="math inline">\(\nabla_\theta \log  P(\tau^{(i)}|\pi_\theta) = \sum^k_{t=0} \nabla_\theta \log \pi_\theta (a_t^{(i)}|s_t^{(i)} ) \)</span>. This only requires gradient of the sum of probabilities of actions given states over the trace, which can be acquired very simply using any of the modern automatic differentiation libraries (i.e. gradient at action probability output from the policy network).</p>
<p>The problem of all traces becoming more likely if rewards are positive is dealt with using the concept of a baseline. The current formulation of <span class="math inline">\(\hat{g}\)</span> is unbiased, i.e. <span class="math inline">\(\mathbb{E}[\hat{g}] = U(\pi_\theta)\)</span> however in the original REINFORCE paper<span class="citation">\cite{Sutton1999a}</span>, it was shown that the following is also unbiased:</p>
<p><span class="math display">\[\nabla_\theta  U(\pi_\theta) \approx \hat{g} = \frac{1}{k} \sum_{i=1}^k \nabla_\theta \log P(\tau^{(i)}|\pi_\theta) \left( R(\tau^{(i)}) - b\right)\]</span></p>
<p>In other words, we can subtract a constant from our weighting term and not affect the bias of our estimator, but may significantly decrease the variance. Specifically, this constant must not be dependent on the actions taken. The reason for this is not obvious, however this result is at the core of many methods that improve upon Policy Gradients (A3C, PPO etc.). If we assume our original estimator is unbiased (this is shown in <span class="citation">\cite{Williams1992}</span>), then we want to show that the component of our estimator that depends on <span class="math inline">\(b\)</span> goes to 0 as the number of traces increases. In other words, the addition of <span class="math inline">\(b\)</span> has no effect on the expected value of our gradients;</p>
<p><span class="math display">\[\begin{aligned}
\mathop{\mathbb{E}}_{\tau \sim \pi_\theta}\left[\hat{g}|_{\text{due to b}}\right] &amp;= \mathop{\mathbb{E}}_{\tau  \sim \pi_\theta} \left[\nabla_\theta \log P(\tau^{(i)}|\pi_\theta) \left( b\right)  \right] &amp;&amp; \text{shifting back from estimate to expectation}\\
&amp;= \sum_{\tau} \left[\nabla_\theta P(\tau^{(i)}|\pi_\theta) \left( b\right)  \right] &amp;&amp; \text{reverse log derivative trick}\\
&amp;= \sum_{\tau} b \left[\nabla_\theta P(\tau^{(i)}|\pi_\theta)  \right] &amp;&amp; \text{only possible if $b$ does not depend on $\theta$}\\
&amp;= \left(\sum_{\tau} b\right)   \nabla_\theta\sum_{\tau \sim \pi_\theta} \left[ P(\tau^{(i)}|\pi_\theta)  \right] &amp;&amp; \text{sum of grads = grad of sums}\\
&amp;= \left(\sum_{\tau} b\right)   \nabla_\theta 1  &amp;&amp; \text{sum of probabilities}\\
&amp;= 0  &amp;&amp; \text{grad of constant}\\
\end{aligned}\]</span></p>
<p>This result hinges on the ability to move <span class="math inline">\(b\)</span> out from the gradient operator, which is only possible if <span class="math inline">\(b\)</span> is some value that does not change with <span class="math inline">\(\theta\)</span>. This is equivalent to stating at the state-action level that <span class="math inline">\(b\)</span> cannot depend on actions. The way the gradient term vanishes may seem surprising, but consider what is actually being calculated. We want to know how a parameterization effects a weighted sum of probabilities - once the weighted term is removed the sum of a probability distribution cannot change, it will always be one. Hence changing <span class="math inline">\(\theta\)</span> has no effect.</p>
<p>Following this result, the estimator can be written explicitly in expanded form as;</p>
<p><span class="math display">\[\nabla_\theta  U(\pi_\theta) \approx \hat{g} = \frac{1}{m} \sum_{i=1}^m \sum_{t=0}^{H-1} \nabla_\theta \log \pi_\theta (a_t^{(i)}|s_t^{(i)} ) \left( \sum^{H-1}_{k=t} R(s_k^{(i)}, a_k^{(i)}) - b(s_t^{(i)})\right)\]</span></p>
<p>Note that here the <span class="math inline">\(i\)</span> indexes the trace, and <span class="math inline">\(k\)</span>, <span class="math inline">\(t\)</span> are the current time-steps within a trace. <span class="math inline">\(H\)</span> indicates that the roll-out will continue to the ‘horizon’ of the episode. As well as substituting the new trace probability, and expanding into state-action form, the above estimator also discards those rewards accumulated before the current action (the <span class="math inline">\(k=t\)</span> term in the last sum). This is somewhat appealing intuitively, as we would like to consider state-action pair in the context of their future, as opposed to past, reward. Practically, this also has the effect of reducing the variance of the estimator.</p>
<p>We are still left with the problem of what to use for <span class="math inline">\(b(s_t^{(i)})\)</span>. There are many options here, but for vanilla policy gradients we do what seems intuitive and subtract rewards by the total average reward <span class="math inline">\(\mathbb{E}[r_0 + r_1 + ... + r_H | s_0 = s]\)</span>. This expectation could be taken over different scales, but for Vanilla PG, it is done over all trajectories and timesteps in the current set of trajectories (larger the set of trajectories, lower the variance). This expectation is actually an approximation of the state-value <span class="math inline">\(V_\pi(s)\)</span>, and when considered in this context, we can reduce the previous estimator (by using higher level notation) into <span class="math inline">\(\hat{g} = \sum_t \nabla_\theta \log \pi_\theta (a_t|s_t)\hat{A}
\)</span> where <span class="math inline">\(\hat{A} = R_t - V_t\)</span>. It can be seen how this form omits some details however.</p>
<p>In practice, it is common to use a library that provides automatic differentiation. In this case we define a loss function where differentiating this loss function w.r.t <span class="math inline">\(\theta\)</span> results in the estimator. <span class="math display">\[L(\theta) = \hat{\mathbb{E}} \left[ \log \pi_\theta (a_t | s_t)\hat{A}_t  \right]\]</span></p>
<p>For Vanilla Policy Gradients, we take one update step each time the estimate is calculated.</p>
<h3 id="continuous-action-spaces">Continuous Action Spaces</h3>
<p>One of the major constraints of both value-based methods and VPG is the difficulty with which they scale to large action spaces. It is not trivial to apply these algorithms to problems with a continuous state space, such robotic joint control. A naive approach is to simply discretized the action space, however Silver et. al. <span class="citation">\cite{Silver2014}</span> demonstrated that effective continuous control could be achieved via Deterministic Policy Gradients (DPG). In this formulation, instead of learning a parameterized distribution across actions, <span class="math inline">\(\pi_\theta (a_t|s_t)\)</span>, a deterministic policy, <span class="math inline">\(a = \mu_\theta\)</span> is learned. This deterministic policy is then combined with Gaussian noise for exploration. This makes DPG an off-policy method, unlike VPG which is on-policy. A key result of this work is the Deterministic Policy gradient theorem; <span class="math display">\[\begin{aligned}
\nabla_\theta J(\mu_\theta) &amp;= \int_\mathcal{S} \rho^\mu (s)\nabla_\theta \mu_\theta(s)\nabla_{\mu_\theta(s)} A(s,\mu_\theta(s)) ds\\
&amp;= \mathbb{E}_{s \sim \rho^\mu} \left[ \nabla_\theta \mu_\theta(s) \nabla_{\mu_\theta(s)} A(s,\mu_\theta(s)) \right] 
\end{aligned}\]</span></p>
<p>Where <span class="math inline">\(\rho^\mu\)</span> is the ‘occupancy measure’, or state visitation probability under <span class="math inline">\(\mu\)</span> and <span class="math inline">\(A\)</span> is some advantage function. In the original paper, <span class="math inline">\(\mu_\theta\)</span> was implemented with a linear approximator. Visually, we can imagine <span class="math inline">\(\mu_\theta\)</span> as a set of distributions over the states, with each state-action pair having it’s own ‘true’ optimal Q-Value equal to the total expected reward from following the optimal policy. Note that the Q-distribution is policy dependent, i.e. the current values depend on the actions made in the future. In this context, DPG can be viewed as a divergence-minimization in <span class="math inline">\(h\)</span> dimensions, with each state distribution being analogous to a component of the joint distribution. A key difference to the figure, however is that the target distribution is sampled from the current policy. In other words we have no ‘ground truth’ which we can sample and try to fit - any sample we take is influenced by our exploration policy. For example, we might have a policy that does excellent moves early on, but is consistently terrible at the end. If rewards come at the end of an episode, then these initial, good moves, are actually negatively reinforced, and so the parameterization moves away from this optima.</p>


<p>Recently, Stochastic Policy Gradients (VPG/SPG) and Deterministic Policy Gradients (DPG) have been shown to be specific instances of a more general Expected Policy Gradient theorem (EPG)<span class="citation">\cite{Ciosek2017}</span>. Research into this area is ongoing.</p>
<p>Standard DPG was extended to Neural Network policy parameterization 2015<span class="citation">\cite{Lillicrap2015}</span>, termed Deep Deterministic Policy Gradients (DDPG). This work adapted many of the ideas from DQN’s to function in the deterministic PG setting, specifically replay memory and target networks, as well as batch normalization<span class="citation">\cite{Ioffe2015}</span>.</p>
<p>When we consider the vanilla policy gradient approach however, it is clear that significant amounts of potentially useful information is lost on each update; every sampled trace is combined into a single parameter update and then discarded. The question that then arises is how to make better use of the information we have obtained - which is often the most costly part of the RL process - in order to learn quicker or more stably. A3C (Asynchronous Advantage Actor Critic) <span class="citation">\cite{Mnih2016a}</span> takes a step towards this goal by using a fixed-horizon advantage estimator, implemented as a separate neural network referred to as the ‘critic’. In practice it is common for the advantage estimator (critic) and cost estimator (actor) to share initial layers that serve as feature extractors. At time of it’s publication, A3C set a new stat of the art for ATARI games, as well a new, 3D navigation testbed proposed in the paper, termed ‘labyrinth’.</p>